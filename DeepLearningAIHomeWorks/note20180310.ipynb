{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本文是[神经网络和深度学习](http://mooc.study.163.com/course/2001281002#/info)的课程笔记，本文的标题号和课程号保持一致，以方便查阅。\n",
    "\n",
    "# 2.1 样本的表示\n",
    "\n",
    "题目的目标是判断图片中是否有猫，假设一张图是64×64像素，3通道，即R、G、B分别对应一个64×64的矩阵：\n",
    "\n",
    "R = $\\left\\lgroup \\matrix{R_{1\\,1} & R_{1\\,2} & ... & R_{1\\,64} \\cr R_{2\\,1} & R_{2\\,2} & ... & R_{2\\,64} \\cr ... & ... & ... \\cr R_{64\\,1} & R_{64\\,2} & ... & R_{64\\,64} } \\right \\rgroup$ $\\;$ G = $\\left\\lgroup \\matrix{G_{1\\,1} & G_{1\\,2} & ... & G_{1\\,64} \\cr G_{2\\,1} & G_{2\\,2} & ... & G_{2\\,64} \\cr ... & ... & ... \\cr G_{64\\,1} & G_{64\\,2} & ... & G_{64\\,64} } \\right \\rgroup$ $\\;$  B = $\\left\\lgroup \\matrix{B_{1\\,1} & B_{1\\,2} & ... & B_{1\\,64} \\cr B_{2\\,1} & B_{2\\,2} & ... & B_{2\\,64} \\cr ... & ... & ... \\cr B_{64\\,1} & B_{64\\,2} & ... & B_{64\\,64} } \\right \\rgroup$ \n",
    "\n",
    "需要将该样本转换成一个一维数组，以方便后面的运算。因此将该样本转换成：\n",
    "\n",
    "x = $\\left\\lgroup \\matrix{R_{1\\,1} \\cr R_{1\\,2} \\cr ... \\cr R_{2\\,1} \\cr R_{2\\,2} \\cr ... \\cr G_{1\\,1} \\cr G_{1\\,2} \\cr ... \\cr B_{1\\,1} \\cr B_{1\\,2} \\cr ... } \\right \\rgroup$ 它包含64×64×3=12288个元素，写作n=$n_{x}$=12288，这是样本的维数。\n",
    "\n",
    "\n",
    "\n",
    "假设共有m个样本，样本空间可以写作：\n",
    "\n",
    "$X=\\begin{bmatrix}\\mid&\\mid&&\\mid\\\\x^{(1)}&x^{(2)}&\\cdots&x^{(m)}\\\\ \\mid&\\mid&&\\mid\\end{bmatrix} , Y=[y^{(1)} y^{(2)} ... y^{(m)}] \\;$ \n",
    "\n",
    "其中: $x^{(i)}\\in \\mathbb{R}^{n_{x}},\\;\\;\\; y^{(i)}\\in \\{0, 1\\}, \\;\\;\\; X\\in\\mathbb{R}^{n_{x}×m}，\\;\\;\\;  X.shape=(n_{x}, m)，\\;\\;\\;  Y\\in \\mathbb{R}，\\;\\;\\; Y.shape=(1, m) $\n",
    "\n",
    "# 2.2 逻辑回归问题\n",
    "\n",
    "对一张图片判猫的问题是一个逻辑回归问题，即给定x，求解ŷ=P(y=1|x)，即y=1的概率。\n",
    "\n",
    "对于图片$x\\in(n_{x}, 1)$，可令$ŷ=σ(w^{T}x+b)$， 其中$w\\in\\mathbb{R}^{n_{x}}, b\\in\\mathbb{R}$\n",
    "\n",
    "该函数将图片中的像素点映射为介于(0, 1)之间的一个数字，该数字就表示其中是否包含猫的概率，函数的图像如下：\n",
    "\n",
    "<img src=\"./images/sigmoid.png\" width=\"400\" alt=\"\" align=center />其中$z=w^{T}x+b ，\\; \\; \\; σ(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "当$z->+\\infty, \\; σ(z)≈\\frac{1}{1+0}=1$\n",
    "\n",
    "当$z->-\\infty, \\; σ(z)≈\\frac{1}{1+\\infty}=0$\n",
    "\n",
    "于是本节题目就是要找到合适的w和b使得对于所有样本x，计算出的ŷ都能逼近y。\n",
    "\n",
    "# 2.3 逻辑回归的损失函数和成本函数\n",
    "\n",
    "对于给定样本$ŷ=σ(w^{T}x+b) \\;, where \\, σ(z)=\\frac{1}{1+e^{-z}}$定义损失函数\n",
    "\n",
    "$L(ŷ, y)=\\frac{1}{2}(ŷ-y)^{2}$\n",
    "\n",
    "只需要找到令成本函数值最小的w和b即可。但由于该函数是个非凸函数，找它的全局最优解比较困难，通常采用变通的方式，令损失函数\n",
    "\n",
    "$L(ŷ, y)=-(y\\logŷ + (1-y)\\log(1-ŷ))$\n",
    "\n",
    "当y=1时$L(ŷ, y)=-\\logŷ$， 希望L(ŷ, y)非常小，这就要求$\\logŷ$足够大，这要求ŷ足够大，但ŷ是σ函数，最大也就无限接近1；\n",
    "\n",
    "当y=0时$L(ŷ, y)=-\\log(1-ŷ)$， 希望L(ŷ, y)非常小，这就要求$\\log(1-ŷ)$足够大，这要求ŷ足够小，但ŷ是σ函数，最大也就无限接近0；\n",
    "\n",
    "从上面的趋势分析上来看，ŷ无限接近y和L(ŷ, y)足够小的趋势是一致的。\n",
    "\n",
    "> 我的问题是：应该让损失函数L(ŷ, y)趋近于0，而不是无限小到负数吧？因为从直观意义上来看损失函数表示ŷ和y之间的差距，太大和太小都不对呀？\n",
    "\n",
    "损失函数是作为单样本而言，对于整个样本空间，则要考察成本函数\n",
    "\n",
    "$J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m} [y^{(i)}\\logŷ^{(i)} + (1-y^{(i)})\\log(1-ŷ^{(i)})]$\n",
    "\n",
    "# 2.4 梯度下降法\n",
    "\n",
    "对于一维凸函数\n",
    "<img src=\"./images/GradientDescent.png\" width=\"400\" alt=\"\" align=center />\n",
    "Repeat{\n",
    "\n",
    "$\\;  w := w - \\alpha\\frac{dJ_{(w)}}{dw}$\n",
    "  \n",
    "}\n",
    "\n",
    "先随机找一个点，计算函数在此处的导数，再不断重复运算$w := w - \\alpha\\frac{dJ_{(w)}}{dw}$直到函数与前一个点的差值小于某个阈值。α叫做学习率，是每次迭代的步长。通过这种方法找到函数的最小值，这就是梯度下降法。\n",
    "\n",
    "对于成本函数，它有多个变量，与一维函数不同之处在于原来的求导运算要改成求偏导，原理和一维函数是一样的。对于J(w, b)：\n",
    "\n",
    "Repeat{\n",
    "\n",
    "$\\; w := w - \\alpha\\frac{dJ(w, b)}{dw}$\n",
    "\n",
    "$\\; b := b - \\alpha\\frac{dJ(w, b)}{db}$\n",
    "\n",
    "}\n",
    "\n",
    "> 对于一维函数，为啥不让α对应一个几何意义？我直观理解认为α本应是弦长，但不幸的是$\\frac{dJ(w)}{dw}$是斜率而不是cos！因此所谓的步长α只是一个数值，其乘数$\\frac{dJ(w)}{dw}$也只表明一个大致的方向。\n",
    "\n",
    "# 2.8 导数计算之计算图\n",
    "\n",
    "> 本节首次提到了“正向传播”和“反向传播”，正向传播就是将各变量组织成复合函数，反向传播就是对复合函数逐级求导。这么简单的过程为什么称为“正向传播算法”和“反向传播算法”呢？\n",
    "\n",
    "# 2.9 逻辑回归中的梯度下降法\n",
    "\n",
    "给定\n",
    "\n",
    "$z = w^{T}x+b$  ... ①\n",
    "\n",
    "$ŷ = a = σ(z)=\\frac{1}{1+e^{-z}}$ ... ②\n",
    "\n",
    "$L(a, y) = -(y\\log(a) + (1-y)\\log(1-a))$ ... ③\n",
    "\n",
    "接下逐级组织复合函数：\n",
    "<img src=\"./images/note20180310-1.png\" width=\"400\" alt=\"\" align=center />\n",
    "\n",
    "再利用计算图求导：\n",
    "④ $da = \\frac{dL(a, y)}{da} = -\\frac{y}{a} + \\frac{(1-y)}{(1-a)}$\n",
    "\n",
    "⑤ $dz = \\frac{dL}{dz} = \\frac{dL}{da} \\frac{da}{dz} $\n",
    "\n",
    " 　　$ = (-\\frac{y}{a} + \\frac{(1-y)}{(1-a)}) (-1)(1+e^{-z})^{-2}(-1)e^{-z} $ 　　【将④代入$\\frac{dL}{da}$，并对②求导】\n",
    "\n",
    " 　　$ = (-\\frac{y}{a} + \\frac{(1-y)}{(1-a)}) \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}$\n",
    "\n",
    " 　　$ =(-\\frac{y}{a} + \\frac{(1-y)}{(1-a)})a(1-a)$ 　　　　　【将②代入】\n",
    "\n",
    " 　　$ =-y(1-a) + a(1-y) \\\\ =a - y$\n",
    "\n",
    "⑥ $dw_{1} = \\frac{dL}{dw_{1}} = \\frac{dL}{dz} \\frac{dz}{dw_{1}} = dz x_{1} = x_{1} dz$\n",
    "\n",
    " 　$dw_{2} = x_{2} dz$\n",
    "\n",
    " 　$db = dz$\n",
    "  \n",
    "\n",
    "将⑥代入梯度下降法得到：\n",
    "\n",
    "Repeat{\n",
    "\n",
    "　　$w_{1} := w_{1} - α · dw_{1}$\n",
    "\n",
    "　　$w_{2} := w_{2} - α · dw_{2}$\n",
    "\n",
    "　　$b := b - α · db$\n",
    "\n",
    "}\n",
    "\n",
    "# 2.10 有m个样本的梯度下降法\n",
    "\n",
    "结合上一节结论，当有m个样本时，成本函数：\n",
    "\n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}L(a^{(i)}, y^{(i)})$\n",
    "\n",
    "其中$a^{(i)} = ŷ^{(i)} = σ(z^{(i)} = σ(w^Tx^{(i)} + b)$\n",
    "\n",
    "于是$\\frac{dJ(w,b)}{dw} = \\frac{1}{m}\\sum_{i=1}^{m}\\frac{d}{dw}L(a^{(i)}, y{(i)})$\n",
    "\n",
    "应用梯度下降的伪代码表为：\n",
    "\n",
    "$J=0; dw_{1}=0; dw_{2}=0; db=0;$\n",
    "\n",
    "For i=1..m   　　 【循环I】\n",
    "\n",
    "　$z^{(i)}=w^{T}x^{(i)}+b$\n",
    " \n",
    "　$a^{(i)} = σ(z^{(i)})$\n",
    " \n",
    "　$J += -[y^{(i)}\\log a^{(i)} + (1-y^{(i)})\\log(1-a^{(i)})]$\n",
    " \n",
    "　$dz^{(i)} = a^{(i)} - y^{(i)}$ 　　【将⑤代入】\n",
    " \n",
    "　$dw_{1} += {x_{1}}^{(i)}dz^{(i)}$  　 【将⑥代入】 【循环II】\n",
    " \n",
    "　$dw_{2} += {x_{2}}^{(i)}dz^{(i)}$\n",
    " \n",
    "　$db += dz^{(i)}$\n",
    " \n",
    "$J=J/m; \\; dw_{1}=dw_{1}/m; \\; dw_{2}=dw_{2}/m; \\; db=db/m;$\n",
    "\n",
    "$w_{1} := w_{1} - α·dw_{1}$\n",
    "\n",
    "$w_{2} := w_{2} - α·dw_{2}$\n",
    "\n",
    "$b := b - α·db$\n",
    "\n",
    "这只应用了一轮梯度下降，其中包含两个循环：循环I是遍历所有样本，循环II是遍历每个样本里的所有特征。\n",
    "\n",
    "# 2.11向量化\n",
    "\n",
    "理论已经讲完了，剩下的工作就是如何通过向量化消除前一小节中的两个循环，让代码跑得更有效率。\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(100000)\n",
    "b = np.random.rand(100000)\n",
    "\n",
    "c = np.dot(a, b)    # ①\n",
    "\n",
    "for i in range(100000):  # ②\n",
    "    c += a[i]*b[i]\n",
    "```\n",
    "代码①和②在结果上是等价的，前者就是向量化的写法，更简洁且更有效率。\n",
    "\n",
    "对于$z=w^{T}x+b$，其中$w=\\left\\lgroup \\matrix{w_{1} \\cr w_{2} \\cr ... \\cr w_{n_{x}} }\\right \\rgroup$，$x=\\left\\lgroup \\matrix{x_{1} \\cr x_{2} \\cr ... \\cr x_{n_{x}} }\\right \\rgroup$， $w\\in\\mathbb{R}^n_{x}, \\;x\\in\\mathbb{R}^n_{x}$\n",
    "\n",
    "求解z非向量化的写法：\n",
    "``` python\n",
    "z=0\n",
    "for i in range(nx):\n",
    "    z += w[i] * x[i]\n",
    "z += b\n",
    "```\n",
    "向量化的写法只需要：\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "```\n",
    "\n",
    "CPU和GPU都有并行化的指令，又叫SIMD（单指令流多数据流 single instruments multiple data），在python中有使用SIMD的内置函数，如`np.function`，GPU更擅长执行SIMD。\n",
    "\n",
    "# 2.12 更多向量化的例子\n",
    "尽量使用np的向量画函数来取代for循环，例如：\n",
    "\n",
    "$v=\\left\\lgroup \\matrix{v_{1} \\cr v_{2} \\cr ... \\cr v_{n} }\\right \\rgroup$，求$u=\\left\\lgroup \\matrix{e^{v_{1}} \\cr e^{v_{2}} \\cr ... \\cr e^{v_{n}} }\\right \\rgroup$\n",
    "\n",
    "``` python\n",
    "u = np.zeros((n, 1))\n",
    "for i in range(n):    # 非向量化的写法\n",
    "    u[i] = math.exp(v[i])\n",
    "    \n",
    "u = np.exp(v)      # 向量化的写法\n",
    "```\n",
    "接下来利用向量化将2.10伪码中的for循环消除掉：\n",
    "\n",
    "$J=0;$  \n",
    "\n",
    "$dw_{1}=0; dw_{2}=0;$ 　【替换为`dw = np.zeros((n_x, 1))`】\n",
    "\n",
    "$db=0;$\n",
    "\n",
    "For i=1..m   　　 \n",
    "\n",
    "　$z^{(i)}=w^{T}x^{(i)}+b$  　【替换为`Z=W.T * X + b = np.dot(W.T, X) + b`】\n",
    " \n",
    "　$a^{(i)} = σ(z^{(i)})$   　【替换为`A = sigmoid(Z)`】\n",
    " \n",
    "　$J += -[y^{(i)}\\log a^{(i)} + (1-y^{(i)})\\log(1-a^{(i)})]$\n",
    " \n",
    "　$dz^{(i)} = a^{(i)} - y^{(i)}$ 　　【替换为`dZ = A - Y`】\n",
    " \n",
    "　$dw_{1} += {x_{1}}^{(i)}dz^{(i)}$  　 【替换为`dW = (1/m) * X * dZ`】\n",
    " \n",
    "　$dw_{2} += {x_{2}}^{(i)}dz^{(i)}$\n",
    " \n",
    "　$db += dz^{(i)}$  　 【替换为`db = (1/m) * np.sum(dZ)`】\n",
    " \n",
    "$J=J/m;$\n",
    "\n",
    "$ dw_{1}=dw_{1}/m; \\; dw_{2}=dw_{2}/m; $   　 【替换为`dW /= m`】\n",
    "\n",
    "$db=db/m;$\n",
    "\n",
    "$w_{1} := w_{1} - α·dw_{1}$\n",
    "\n",
    "$w_{2} := w_{2} - α·dw_{2}$\n",
    "\n",
    "$b := b - α·db$\n",
    "\n",
    "# 2.13 有m个样本的向量化逻辑回归\n",
    "\n",
    "对于有m个样本：\n",
    "\n",
    "$z^{(1)} = w^{T}x^{(1)} + b \\; \\; \\; z^{(2)} = w^{T}x^{(2)} + b \\; \\; ... \\; z^{(m)} = w^{T}x^{(m)} + b \\; \\; \\; $\n",
    "\n",
    "$a^{(1)}=σ(z^{(1)}) \\; \\; \\; \\; \\; \\; \\; a^{(2)}=σ(z^{(2)}) \\; \\; \\; \\; \\; \\; ... \\; a^{(m)}=σ(z^{(m)}) \\; \\; \\; $\n",
    "\n",
    "可令$X=\\begin{bmatrix}\\mid&\\mid&&\\mid\\\\x^{(1)}&x^{(2)}&\\cdots&x^{(m)}\\\\ \\mid&\\mid&&\\mid\\end{bmatrix} \\; \\in \\mathbb{R}^{n_{x}×m}$ \n",
    "\n",
    "则$Z = [Z^{(1)} \\; Z^{(2)} \\; ... \\; Z^{(m)}] = W^{T}·X + [b, b, ..., b] $\n",
    "\n",
    "使用向量化的代码写作\n",
    "``` python\n",
    "Z = np.dot(W.T, X) + b\n",
    "```\n",
    "实数b被自动扩展为矩阵，这在python中被称作广播。\n",
    "\n",
    "$A = [a^{(1)}, a^{(2)}, ..., a^{(m)}] = σ(Z)$\n",
    "\n",
    "# 2.14 向量化逻辑回归成本函数的梯度下降法\n",
    "继续向量化2.12中的伪码：\n",
    "\n",
    "$A = [a^{(1)}, a^{(2)}, ..., a^{(m)}] \\; \\; \\; Y = [y^{(1)}, y^{(2)}, ..., y^{(m)}]$\n",
    "\n",
    "$Z = W^{T}X + b = np.dot(W.T, X) + b$\n",
    "\n",
    "$A = σ(Z)$\n",
    "\n",
    "$dZ = A - Y$\n",
    "\n",
    "$dW = \\frac{1}{m}XdZ^{T}$\n",
    "\n",
    "$db = \\frac{1}{m} np.sum(dZ)$\n",
    "\n",
    "$W := W - α·dW$\n",
    "\n",
    "$b := b - α·db$\n",
    "\n",
    "以上是一次梯度下降的迭代，若要进行n轮迭代，还是需要一个n的循环，这个最外层循环是无法避免的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
